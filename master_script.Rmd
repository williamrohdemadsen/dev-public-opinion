---
title: "dev-public-opinion"
author: "William Rohde Madsen"
output: html_document
---

# Set up
```{r set-up}
# Load packages
library(tidyverse)
library(devtools)
library(readxl)
library(lubridate)
library(stringr)
library(janitor)
library(zoo)
library(RcppRoll)
library(rgdal) # for shapefiles
library(sf) # manipulating spatial data
library(lme4) # for glmer, etc. modelling
library(httr)
library(ggrepel)
library(reticulate) # using python code in R
library(vroom) # loading json faster
library(ndjson)
library(jsonlite)
library(rvest) # reading html websites
library(maps)
library(ggmap)
library(stars) # rasters
library(maptools)
library(spatstat)
library(koRpus)
library(quanteda)
library(textdata)
library(tidytext)
library(SnowballC)

# Load package functions
devtools::load_all()

```

# Load prepared data
```{r load-data}
# Load formatted data
load("data/formatted_data.RData")

```

# Sentiment analysis
```{r prepare-text}
# Clean from patterns and turn to lower-case
tweets_clean <- tweets_formatted %>%
  as.data.frame() %>%
  select(-c(geometry)) %>%
  tibble() %>%
  mutate(tweet = tolower(tweet),
         tweet = remove_patterns_in_tweet(tweet),
         tweet = str_squish(tweet) # removes repeated white space
  )

# Save stopwords
stop_words <- get_stopwords(language = "en", source = "snowball") %>%
  tibble() %>%
  transmute(word, stem = SnowballC::wordStem(word))

# Convert to tokens and stem
tweets_tokens <- tweets_clean %>%
  unnest_tokens(word, tweet) %>%
  mutate(stem = SnowballC::wordStem(word),
         stop_word = if_else(word %in% stop_words$word | word %in% stop_words$stem,
                             TRUE, FALSE)
         )



```

```{r join-lexicon}
# Unique words to join as factors for speed
stem_lvls <- unique(c(tweets_tokens$stem, afinn_stem$stem))

# Join lexicon
senti_tweets <- tweets_tokens %>%
  mutate(stem = factor(stem, stem_lvls)) %>%
  left_join(.,
            afinn_stem,
            by = "stem") %>%
  arrange(country, region_1, leader, date)

# Share of tweets with no lexicon match for any of their stemmed words
(share_tweets_no_senti <- senti_tweets %>%
  group_by(id, username, language) %>%
  summarise(afinn_mean = mean(afinn_value, na.rm = TRUE)) %>%
  group_by(is.na(afinn_mean), language == "en") %>%
  summarise(n = n())
  )

```

# Raw estimates
```{r raw-estimates}
# Mean sentiment per tweet
senti_mean <- senti_tweets %>%
  group_by(id, username, date, week = floor_date(date, "week"), country, region_1, leader) %>%
  summarise(afinn_mean = mean(afinn_value, na.rm = TRUE)) %>%
  ungroup() %>%
  filter(!is.na(afinn_mean)) %>% # drop lexicon NAs
  mutate(binary = if_else(afinn_mean < 0, 0, 1)) # if sentiment is negative, binary is 0

# Mean sentiment per week
senti_mean %>%
  group_by(week, country, region_1, leader) %>%
  summarise(afinn_mean = mean(afinn_mean, na.rm = TRUE)) %>%
  group_by(country, region_1, leader) %>%
  mutate(afinn_roll = roll_mean(afinn_mean, n = 7, fill = TRUE)) %>%
  filter(leader == "Buhari") %>%
  ggplot(., aes(x = week, y = afinn_mean)) +
  geom_point() +
  geom_line(aes(y = afinn_roll)) +
  facet_wrap(~region_1)

# Plot mean rating per day
ggplot(senti_mean_day, aes(x = date, y = afinn_value)) +
  geom_point(aes(colour = leader)) +
  facet_wrap(~country)


```



# Train model
```{r train-model}

```


# Plots
```{r plot}
# Simply GADM for plotting ease
gadm_simp <- gadm %>%
  st_simplify(., dTolerance = 0.1)

# Plots

# English-speakers plot ------
# Plot different data on English speakers
supp %>%
  ggplot(.,
         aes(x = eng_prop_wiki,
             y = eng_prop) # ethno
  ) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "Different sources on English speakers per country")

# Plot English speaking population against corruption index
supp %>%
  #filter(wgi_est < 1) %>%
  filter(twitter_users_pc < 0.3) %>%
  ggplot(.,
         aes(x = twitter_users_pc,
             y = eng_prop)) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "English-speaking share by voice and accountability index")

# Raw tweets plots -----
# Plot number of tweets every week
tweets %>%
  filter(country == "Nigeria") %>%
  mutate(week = floor_date(date, unit = "week"),
         month = floor_date(date, unit = "month")
  ) %>%
  group_by(month, country, leader) %>%
  summarise(n = n()) %>%
  ggplot(.,
         aes(x = month,
             y = n)) +
  geom_col() +
  labs(title = "Number of Tweets mentioning Buhari in Lagos, Nigeria",
       subtitle = paste0(nrow(tweets), " tweets from Lagos")
  ) 


# Map Tweets as points
world1 <- map("world", plot = FALSE, fill = TRUE) %>% sf::st_as_sf()
sf::st_as_sf(map("africa", plot = FALSE, fill = TRUE))

# Nigeria NE
nigeria_sf <- world1 %>% filter(ID == "Nigeria")

tweets_n <- nrow(tweets)
tweets_n_w_points <- nrow(tweets[!is.na(tweets$place_coordinates_0),])

ggplot() +
  geom_sf(data = nigeria_sf) +
  coord_sf(xlim = c(8.5, 9.5), ylim = c(7, 7.7), expand = FALSE) +
  geom_point(data = tweets, aes(x = place_coordinates_0, y = place_coordinates_1),
             size = 0.7) +
  labs(title = "Lagos in Nigeria: Tweets which included point spatial data",
       subtitle = paste0(tweets_n_w_points, " of ", tweets_n, " tweets include spatial data")
  ) +
  theme_bw()


# Methodology plots -----
# Plot scraper circles
ggplot() +
  geom_sf(data = gadm_simp) +
  geom_sf(data = gadm_circ, fill = NA)


```

