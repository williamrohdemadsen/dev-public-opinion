---
title: "dev-public-opinion"
author: "William Rohde Madsen"
output: html_document
---

# Set up
```{r set-up}
# Load packages
library(tidyverse)
library(devtools)
library(readxl)
library(lubridate)
library(stringr)
library(janitor)
library(zoo)
library(RcppRoll)
library(rgdal) # for shapefiles
library(sf) # manipulating spatial data
library(lme4) # for glmer, etc. modelling
library(httr)
library(ggrepel)
library(reticulate) # using python code in R
library(vroom) # loading json faster
library(ndjson)
library(jsonlite)
library(rvest) # reading html websites
library(maps)
library(ggmap)
library(stars) # rasters
library(maptools)
library(spatstat)
library(koRpus)
library(quanteda)
library(textdata)
library(tidytext)
library(SnowballC)

# Load package functions
devtools::load_all()

```

# Load prepared data
```{r load-data}
# Format data
#source("data-raw/03_master_formatting.R")

# Load formatted data
load("data/formatted_data.RData")

```

# Sentiment analysis
```{r prepare-text}
# Create tweets tokens
tweets_tokens <- create_tweet_tokens(tweets_sf)

```

```{r join-lexicon}
# Join sentiment values by stem
senti_tweets <- join_sentiment_by_stem(tweet_tokens, afinn_stem)

# Share of tweets with no lexicon match for any of their stemmed words
(share_tweets_no_senti <- senti_tweets %>%
    group_by(id, username, language) %>%
    summarise(afinn_mean = mean(afinn_value, na.rm = TRUE)) %>%
    group_by(is.na(afinn_mean), language == "en") %>%
    summarise(n = n())
)

```

# Raw estimates
```{r tweets-per-weel}
# Summarise tweets per week
tweets_n_per_week <- tweets_sf %>%
  as.data.frame() %>%
  select(-geometry) %>%
  group_by(country, leader, week = floor_date(date, "week")) %>%
  summarise(n = n())

# Mean number of tweets per week
tweets_n_per_week %>%
  summarise(mean = mean(n))

# Plot
ggplot(tweets_n_per_week, aes(x = week, y = n, col = leader)) +
  geom_line() +
  geom_hline(yintercept = 100) +
  #facet_wrap(~ leader) +
  labs(title = "Tweets per week")

```


```{r raw-estimates}
# Mean sentiment per tweet
senti_mean <- create_mean_sentiment_per_tweet(senti_tweets)

# Find for-against based on mean sentiment
senti_cut_offs <- create_cut_offs(senti_mean)

# Create an estimate per week per cut off
raw_pro_shares <- find_pro_share(senti_cut_offs, n_roll = 5)

# Plot pro-share by cut-offs
raw_pro_shares %>%
  filter(leader %in% c("Buhari", "Goodluck Jonathan")) %>%
  ggplot(., aes(x = date, y = pro_share, colour = leader)) +
  geom_point() +
  geom_line(aes(y = pro_share_roll)) +
  facet_wrap(~ cut_off) +
  labs(title = "Raw favourability ratings at different cut-offs")


```


# Train model
```{r train-model}



```

# Validation
```{r validate-raw}
# Join raw estimates to election results
validate_elex_raw <- join_election_to_raw_senti(raw_pro_shares, elex_master)

# Summary statistics of each cut-off
cut_off_stats <- summarise_cut_off_validation(validate_elex_raw)

# Plot raw estimate stats of each cut-off
cut_off_stats %>%
  filter(days_diff < 0) %>%
  ggplot(.,
         aes(x = days_diff, y = abs(mean))) +
  geom_point() +
  geom_smooth(method = lm) +
  facet_wrap(~ cut_off) +
  labs(title = "Mean difference between raw estimate and election results for each cut-off value by days to election")


```



# Plots
```{r plot}
# Simply GADM for plotting ease
gadm_simp <- gadm %>%
  st_simplify(., dTolerance = 0.1)

# Plots

# English-speakers plot ------
# Plot different data on English speakers
supp %>%
  ggplot(.,
         aes(x = eng_prop_wiki,
             y = eng_prop) # ethno
  ) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "Different sources on English speakers per country")

# Plot English speaking population against corruption index
supp %>%
  #filter(wgi_est < 1) %>%
  filter(twitter_users_pc < 0.3) %>%
  ggplot(.,
         aes(x = twitter_users_pc,
             y = eng_prop)) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "English-speaking share by voice and accountability index")

# Raw tweets plots -----
# Plot number of tweets every week
tweets %>%
  filter(country == "Nigeria") %>%
  mutate(week = floor_date(date, unit = "week"),
         month = floor_date(date, unit = "month")
  ) %>%
  group_by(month, country, leader) %>%
  summarise(n = n()) %>%
  ggplot(.,
         aes(x = month,
             y = n)) +
  geom_col() +
  labs(title = "Number of Tweets mentioning Buhari in Lagos, Nigeria",
       subtitle = paste0(nrow(tweets), " tweets from Lagos")
  ) 


# Map Tweets as points
world1 <- map("world", plot = FALSE, fill = TRUE) %>% sf::st_as_sf()
sf::st_as_sf(map("africa", plot = FALSE, fill = TRUE))

# Nigeria NE
nigeria_sf <- world1 %>% filter(ID == "Nigeria")

tweets_n <- nrow(tweets)
tweets_n_w_points <- nrow(tweets[!is.na(tweets$place_coordinates_0),])

ggplot() +
  geom_sf(data = nigeria_sf) +
  coord_sf(xlim = c(8.5, 9.5), ylim = c(7, 7.7), expand = FALSE) +
  geom_point(data = tweets, aes(x = place_coordinates_0, y = place_coordinates_1),
             size = 0.7) +
  labs(title = "Lagos in Nigeria: Tweets which included point spatial data",
       subtitle = paste0(tweets_n_w_points, " of ", tweets_n, " tweets include spatial data")
  ) +
  theme_bw()


# Methodology plots -----
# Plot scraper circles
ggplot() +
  geom_sf(data = gadm_simp) +
  geom_sf(data = gadm_circ, fill = NA)


```

