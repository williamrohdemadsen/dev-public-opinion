---
title: "dev-public-opinion"
author: "William Rohde Madsen"
output: html_document
---

# Set up
```{r set-up}
# Load packages
library(tidyverse)
library(devtools)
library(readxl)
library(lubridate)
library(stringr)
library(janitor)
library(zoo)
library(RcppRoll)
library(rgdal) # for shapefiles
library(sf) # manipulating spatial data
library(lme4) # for glmer, etc. modelling
library(httr)
library(ggrepel)
library(reticulate) # using python code in R
library(vroom) # loading json faster
library(ndjson)
library(jsonlite)
library(rvest) # reading html websites
library(maps)
library(ggmap)
library(stars) # rasters
library(maptools)
library(spatstat)
library(koRpus)
library(quanteda)
library(textdata)
library(tidytext)
library(SnowballC)

# Load package functions
#devtools::load_all()

```

# Load prepared data
```{r load-data}
# Load formatted data
# Formatted with R scripts in data-raw
#load("data/formatted_data.RData")

load("data/formatted_gadm.RData")

load("data/formatted_supplementary.RData")

load("data/tweets_sf.RData")

load("data/tweets_tokens.RData")

#future_map()

# Source R scripts
list.files("R", full.names = TRUE) %>%
  purrr::map(source)

```


# Sentiment analysis
```{r join-senti-to-tokens}
# Join sentiment values by stem
senti_per_token <- join_sentiment_by_stem(tweet_tokens, afinn_stem)

```


```{r senti-per-tweet}
# Calculate sentiment per tweet
# Mean sentiment per tweet
senti_per_tweet <- create_mean_sentiment_per_tweet(senti_per_token)

```


```{r senti-per-day}
# Sentiment per day
senti_per_day <- senti_per_tweet %>%
  group_by(date, has_point, replies_count, retweets_count, likes_count,
           leader, leader_country, country, region_1, region_2) %>%
  summarise(afinn_mean_day = mean(afinn_mean, na.rm = TRUE),
            n_tweets = n()) %>%
  ungroup() %>%
  arrange(date, country, region_1, region_2)

senti_per_day

```

```{r senti-based-on-region}
# Sentiment per region adjusted over time
# Based on regions
# average sentiment per region
# using non-point tweets to show trend over time

# Number of tweets with or without point
senti_per_tweet$has_point %>% table #%>% prop.table

# Calculate average sentiment per region
senti_per_region <- senti_per_tweet %>%
  filter(has_point) %>%
  filter(!is.na(country)) %>% # remove tweets outside of country
  group_by(leader, country, region_1, region_2) %>%
  summarise(afinn_mean_region = mean(afinn_mean, na.rm = TRUE),
            #n_tweets = n()
  ) %>%
  ungroup()

# Join average sentiment per day for trend over time
senti_per_region <- senti_per_day %>%
  left_join(senti_per_region,
            by = c("leader", "country", "region_1", "region_2"))


```


# Raw estimates
```{r raw-estimates}
# Find for-against based on mean sentiment
senti_cut_offs <- create_cut_offs(senti_per_tweet)

# Create an estimate per week per cut off
raw_pro_shares <- find_pro_share(senti_cut_offs, n_roll = 5)

# Plot pro-share by cut-offs
raw_pro_shares %>%
  filter(leader == "Ghani") %>%
  ggplot(., aes(x = date, y = pro_share, colour = leader)) +
  geom_point() +
  geom_line(aes(y = pro_share_roll), colour = "black") +
  facet_wrap(~ cut_off) +
  labs(title = "Raw favourability ratings at different cut-offs")


```


```{r validate-raw}
# Join raw estimates to election results
raw_pro_shares_targets <- join_targets_to_senti(raw_pro_shares, elex_master)

# Summary statistics of each cut-off
cut_off_stats <- summarise_cut_off_validation(raw_pro_shares_targets)

# Plot raw estimate stats of each cut-off
cut_off_stats %>%
  filter(days_diff < 0) %>%
  ggplot(.,
         aes(x = days_diff, y = abs(mean))) +
  geom_point() +
  geom_smooth(method = lm) +
  facet_wrap(~ cut_off) +
  labs(title = "Mean difference between raw estimate and election results for each cut-off value by days to election")


```

# Add targets for modelling
```{r add-targets-for-models}
# Add targets (election results and polls) to fit models

# Join sentiment with target vector, election and polling results
senti_per_day <- senti_per_day %>%
  mutate(target_id = row_number())

senti_targets_all <- join_targets_to_senti(senti_per_day, targets_master)

# Select nearest targets (election or poll) for each day
senti_targets <- select_nearest_target(senti_targets_all)

senti_targets <- senti_targets %>%
  # create variable with country and region_1
  mutate(year = year(date),
         country_and_region_1 = paste0(country, "_", region_1))

```

```{r add-gdl-covariates}
# Add GDL covariates
senti_targets_covars <- left_join(senti_targets, gdl_interpo,
                                  by = c("year", "country", "region_1" = "gadm_region"))

# Create dataset of predictions
predictions <- senti_targets_covars

```


# Poll-based model
```{r poll-model-1}
# Multilevel models trained on polls

# Model 1
# Multilevel regression
train_data_1 <- senti_targets_covars %>%
  # create id to later subset test data
  mutate(id = row_number()) %>%
  filter(type == "poll") %>%
  filter(days_diff_abs < 10) %>%
  # remove Zimbabwe's 2013 election
  filter(!(country == "Zimbabwe" & year(date) == 2013))

model_1_1 <- lmer(votes_share ~ afinn_mean_day + (1|country_and_region_1),
                  data = train_data_1)

model_1_2 <- lmer(votes_share ~ afinn_mean_day + retweets_count + likes_count + (1|country_and_region_1),
                  data = train_data_1)

model_1_3 <- lmer(votes_share ~ afinn_mean_day + days_diff_abs + retweets_count + likes_count + (1|country_and_region_1),
                  data = train_data_1)

model_1_4 <- lmer(votes_share ~ afinn_mean_day + days_diff_abs + phone + (1|country_and_region_1),
                  data = train_data_1)


```

```{r validate-model-1}
# Validate poll-based models

# Predict model 1
predictions_1 <- predictions %>%
  mutate(prediction_1_1 = predict(model_1_1, ., allow.new.levels = TRUE),
         prediction_1_2 = predict(model_1_2, ., allow.new.levels = TRUE),
         prediction_1_3 = predict(model_1_3, ., allow.new.levels = TRUE),
         prediction_1_4 = predict(model_1_4, ., allow.new.levels = TRUE)
         
  )

# Calculate MAE on validation data
predictions_1 %>%
  # remove rows already used for test data - not actually needed if we also use type == "election"
  mutate(id = row_number()) %>%
  filter(!id %in% train_data_1$id) %>%
  filter(type == "election") %>%
  filter(days_diff_abs < 10) %>%
  filter(!(country == "Zimbabwe" & year(date) == 2013)) %>%
  mutate(prediction_1_1_diff = abs(prediction_1_1 - votes_share),
         prediction_1_2_diff = abs(prediction_1_2 - votes_share),
         prediction_1_3_diff = abs(prediction_1_3 - votes_share),
         prediction_1_4_diff = abs(prediction_1_3 - votes_share),
  ) %>%
  group_by(country) %>%
  summarise(mae_1_1 = mean(prediction_1_1_diff, na.rm = TRUE),
            mae_1_2 = mean(prediction_1_2_diff, na.rm = TRUE),
            mae_1_3 = mean(prediction_1_3_diff, na.rm = TRUE),
            mae_1_4 = mean(prediction_1_4_diff, na.rm = TRUE)
  )



```

```{r poll-model-2}
# Model 2
# Multilevel regression and post-stratification (MRP)
model_2_1 <- lmer(votes_share ~ afinn_mean_day + (1|country_and_region_1),
                  data = train_data_1)

summary(model_2_1)

# Predict on test data
predictions_2 <- predictions %>%
  filter(type == "election") %>%
  filter(days_diff_abs < 10) %>%
  filter(!(country == "Zimbabwe" & year(date) == 2013)) %>%
  mutate(prediction_2_1 = predict(model_2_1, ., allow.new.levels = TRUE)
  )

# Post-stratify using GDL's population share
predictions_2 %>%
  transmute(prediction_2_1, popshare/100, prediction_2_1_weighted = prediction_2_1*popshare/100)



```


```{r roc-curve}
# ROC curve

# For example, if difference is less than two percent
# Use existing literature

```


# Tables
```{r table-country-stats}
# Table of country statistics
tweets_df <- tweets_sf %>%
  as.data.frame() %>%
  select(-geometry) %>%
  as_tibble()

# Tweet count per country
(n_tweets_per_country <- tweets_df %>%
    group_by(leader_country) %>%
    summarise(n = n()) %>%
    mutate(variable = "Total tweets per country") %>%
    pivot_wider(names_from = leader_country, values_from = n)
)

# Average number of tweets per country
(n_weekly_tweets_per_country <- tweets_df %>%
    group_by(leader_country, week) %>%
    summarise(n = n()) %>%
    group_by(leader_country) %>%
    summarise(n = mean(n)) %>%
    mutate(variable = "Average number of tweets per country") %>%
    pivot_wider(names_from = leader_country, values_from = n)
)

# Number of unique weeks per country
(n_weeks_per_country <- tweets_df %>%
    group_by(leader_country, week) %>%
    summarise(n = n()) %>%
    group_by(leader_country) %>%
    summarise(n = n()) %>%
    mutate(variable = "Number of weeks per country") %>%
    pivot_wider(names_from = leader_country, values_from = n)
)

# Years per country
(n_years_per_country <- tweets_df %>%
    group_by(leader_country, year(date)) %>%
    summarise(n = n()) %>%
    group_by(leader_country) %>%
    summarise(n = n()) %>%
    mutate(variable = "Number of years per country") %>%
    pivot_wider(names_from = leader_country, values_from = n)
)

# Subnational regions per country
(n_regions_1_per_country <- tweets_df %>%
    group_by(leader_country, region_1) %>%
    summarise(n = n()) %>%
    group_by(leader_country) %>%
    summarise(n = n()) %>%
    mutate(variable = "Number of regions_1 per country") %>%
    pivot_wider(names_from = leader_country, values_from = n)
)

n_regions_1_per_country %>%
  pivot_longer(cols = 2:ncol(.)) %>%
  summarise(sum = sum(value))

# Number of elections per country
(n_elections_per_country <- elex_master %>%
    group_by(country, elex_date) %>%
    summarise(n = n()) %>%
    group_by(country) %>%
    summarise(n = n()) %>%
    mutate(variable = "Number of elections per country") %>%
    pivot_wider(names_from = country, values_from = n)
)

# Number of unique candidacies and leaders
(n_candidates_per_country <- candidates %>%
    group_by(country, name, elex_date) %>%
    summarise(n = n()) %>%
    group_by(country) %>%
    summarise(n = n()) %>%
    mutate(variable = "Number of elections per country") %>%
    pivot_wider(names_from = country, values_from = n)
)

n_candidates_per_country %>%
  pivot_longer(cols = 2:ncol(.)) %>%
  summarise(sum = sum(value))

# Number of polls per country
(n_polls_per_country <- polling_master %>%
    group_by(country, date) %>%
    summarise(n = n()) %>%
    group_by(country) %>%
    summarise(n = n()) %>%
    mutate(variable = "Number of polls per country") %>%
    pivot_wider(names_from = country, values_from = n)
)

# Combine into table
bind_rows(n_tweets_per_country,
          n_weekly_tweets_per_country,
          n_weeks_per_country,
          n_years_per_country,
          n_regions_1_per_country,
          #n_regions_2_per_country,
          n_elections_per_country,
          n_polls_per_country) %>%
  rename(Name = variable)


```


# Descriptive plots

```{r senti-join-stats}
# Share of tweets with no lexicon match for any of their stemmed words
(share_tweets_no_senti <- senti_tweets %>%
   group_by(id, username, language) %>%
   summarise(afinn_mean = mean(afinn_value, na.rm = TRUE)) %>%
   mutate(has_afinn = !is.na(afinn_mean),
          in_english = language == "en") %>%
   group_by(has_afinn, in_english) %>%
   summarise(n = n())
)

```

```{r tweets-per-week}
# Summarise tweets per week
tweets_n_per_week <- tweets_sf %>%
  as.data.frame() %>%
  select(-geometry) %>%
  group_by(country, leader, week = floor_date(date, "week")) %>%
  summarise(n = n()) %>%
  ungroup()

# Mean number of tweets per week
tweets_n_per_week %>%
  group_by(week) %>%
  summarise(mean = mean(n))

# Plot
ggplot(tweets_n_per_week, aes(x = week, y = n, col = leader)) +
  geom_line() +
  geom_hline(yintercept = 100) +
  facet_wrap(~ leader, scales = "free_x") +
  labs(title = "Tweets per week per leader")

# Number of tweets over time
tweets_sf %>%
  as.data.frame() %>%
  group_by(week, has_point) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  ggplot(., aes(x = week, y = n, colour = has_point)) +
  geom_line() +
  facet_wrap(~has_point, scales = "free_y") +
  labs(title = "Number of tweets per week with or without coordinates")

# Number of total tweets with or without coordinates
tweets_sf %>%
  as.data.frame() %>%
  group_by(leader_country, has_point) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  ggplot(., aes(x = has_point, y = n, fill = has_point)) +
  geom_col() +
  facet_wrap(~leader_country, scales = "free_y") +
  labs(title = "Number of total tweets with or without coordinates")

```



# Various plots
```{r plot-maps}
# Simply GADM for plotting ease
gadm_simp <- gadm %>%
  st_simplify(., dTolerance = 0.1)

# Map tweets as points
world1 <- map("world", plot = FALSE, fill = TRUE) %>% sf::st_as_sf()
sf::st_as_sf(map("africa", plot = FALSE, fill = TRUE))

# Nigeria NE
nigeria_sf <- world1 %>% filter(ID == "Nigeria")

tweets_n <- nrow(tweets)
tweets_n_w_points <- nrow(tweets[!is.na(tweets$place_coordinates_0),])

ggplot() +
  geom_sf(data = nigeria_sf) +
  coord_sf(xlim = c(8.5, 9.5), ylim = c(7, 7.7), expand = FALSE) +
  geom_point(data = tweets, aes(x = place_coordinates_0, y = place_coordinates_1),
             size = 0.7) +
  labs(title = "Lagos in Nigeria: Tweets which included point spatial data",
       subtitle = paste0(tweets_n_w_points, " of ", tweets_n, " tweets include spatial data")
  ) +
  theme_bw()

```


```{r plot-eng-speakers}
# English-speakers plot
# Plot different data on English speakers
supp %>%
  ggplot(.,
         aes(x = eng_prop_wiki,
             y = eng_prop) # ethno
  ) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "Different sources on English speakers per country")

# Plot English speaking population against corruption index
supp %>%
  #filter(wgi_est < 1) %>%
  filter(twitter_users_pc < 0.3) %>%
  ggplot(.,
         aes(x = twitter_users_pc,
             y = eng_prop)) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "English-speaking share by voice and accountability index")
```


```{r plot-raw-tweets}
# Raw tweets plots
# Plot number of tweets every week
tweets %>%
  filter(country == "Nigeria") %>%
  mutate(week = floor_date(date, unit = "week"),
         month = floor_date(date, unit = "month")
  ) %>%
  group_by(month, country, leader) %>%
  summarise(n = n()) %>%
  ggplot(.,
         aes(x = month,
             y = n)) +
  geom_col() +
  labs(title = "Number of Tweets mentioning Buhari in Lagos, Nigeria",
       subtitle = paste0(nrow(tweets), " tweets from Lagos")
  ) 

```


```{r plot-methodology}
# Methodology plots
# Plot get circles
ggplot() +
  geom_sf(data = gadm_simp) +
  geom_sf(data = gadm_circ, fill = NA)


```

