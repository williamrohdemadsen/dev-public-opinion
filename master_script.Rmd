---
title: "masterscript"
author: "William Rohde Madsen"
output: html_document
---

# Packages
```{r load-packages, echo = FALSE, include = FALSE}
library(tidyverse)
library(readxl)
library(lubridate)
library(stringr)
library(janitor)
library(rgdal) # for shapefiles
library(sf)
library(lme4) # for glmer, etc. modelling
library(rtweet) # using Twitter API
library(httr)
library(tfse) # to save token to environment
library(ggrepel)
library(reticulate)
library(vroom)
library(ndjson)
library(quanteda)
library(tidytext)
library(rvest)
library(maps)
library(ggmap)

#source("src/theme.R")

```

# Load data
```{r load-supp}
###### Load supplementary data

# Language data from the UN
lang_raw <- read_csv("data/raw/UNdata_Export_20210203_194758725.csv")

# Language data pasted from Ethnologue
ethno_raw <- read_excel("data/raw/ethnologue_english.xlsx")

# Corruption perception index
cpi_raw <- read_excel("data/raw/CPI2020_GlobalTablesTS_210125.xlsx",
                      sheet = "CPI Timeseries 2012 - 2020",
                      skip = 2)

# UN population estimates
## https://population.un.org/wpp/Download/Standard/CSV/
pop_raw <- read_csv("data/raw/WPP2019_TotalPopulationBySex.csv")

# GDP PPP from World Bank
## https://data.worldbank.org/indicator/NY.GDP.MKTP.PP.KD
gdp_ppp_raw <- read_csv("data/raw/API_NY.GDP.MKTP.PP.KD_DS2_en_csv_v2_1928416.csv", skip = 3)

```

```{r load-get-help}
###### Load data to use in collecting Tweets

# Load leader data from REIGN
reign_raw <- read_csv("data/raw/REIGN_2021_2.csv")

# Load geocodes, INITIAL
cap_geo_raw <- read_csv("data/raw/country-capitals.csv")

# Get proxy list, rotating IPs for scraping tweets without getting blocked
## https://free-proxy-list.net/
proxy_raw <- read_html("https://free-proxy-list.net/#")

```

```{r load-subnational}
###### Load subnational data

# Load Global Data Lab data
## https://globaldatalab.org/areadata/download_files/
gdl_raw <- read_csv("data/raw/GDL-AreaData390 (1).csv")

# Subnational infant mortality
## https://sedac.ciesin.columbia.edu/data/set/povmap-global-subnational-infant-mortality-rates-v2/data-download


```

```{r load-shapefiles}
###### Load shapefiles
gdl_shp_raw <- readOGR(dsn = "data/raw/GDL Shapefiles V4 (1)/",
                       layer = "GDL Shapefiles V4")
# st_read("data/raw/GDL Shapefiles V4 (1)/GDL Shapefiles V4.shp")

```


# Format data
```{r format-supp}
###### Format and bind supplementary data
# This will serve to decide and describe the countries in focus

###### Clean
# UN language population
## Appears to only show population by primary language
## May use rural, urban distinctions for weighting
lang_raw %>%
  clean_names() %>%
  filter(language %in% c("Total", "English")) %>%
  mutate(value = round(value)) %>%
  pivot_wider(names_from = language, values_from = value) %>%
  clean_names() %>%
  mutate(english_prop = english/total %>% round) %>%
  rename(country = country_or_area) %>%
  group_by(country, area, sex) %>%
  filter(year == max(year))

# Ethnologue language data, English speakers
ethno <- ethno_raw %>%
  mutate(country = if_else(grepl("Hide Details", English), gsub("Hide Details", "", English), NA_character_),
         country = if_else(English == "English", "United Kingdom", country),
         name = lag(English),
  ) %>%
  fill(country, name) %>%
  filter(lag(English) %in% c("User Population", "Location", "Language Status", "Other Comments")) %>%
  pivot_wider(names_from = name, values_from = English) %>%
  clean_names() %>%
  mutate(eng_total = gsub(" .+", "", user_population) %>% gsub(",", "", .) %>% as.integer,
         l1 = if_else(grepl("L1", user_population),
                      gsub(".+ L1 users: ", "", user_population),
                      NA_character_),
         l1_src = str_extract(l1,  "(?<=\\().+?(?=\\))"),
         l1 = sub(" .+", "", l1) %>% gsub(",", "", .) %>% as.integer,
         l1_yr = str_extract(l1_src, "\\d+") %>% as.integer,
         l2 = if_else(grepl("L2", user_population),
                      gsub(".+ L2 users: ", "", user_population),
                      NA_character_),
         l2_src = str_extract(l2,  "(?<=\\().+?(?=\\))"),
         l2 = sub(" .+", "", l2) %>% gsub(",", "", .) %>% as.integer,
         l2_yr = str_extract(l2_src, "\\d+") %>% as.integer,
         total_yr = if_else(l2 > l1, l2_yr, l1_yr),
         total_yr = if_else(is.na(total_yr), l2_yr, total_yr) %>% if_else(is.na(.), l1_yr, .),
         total_src = if_else(is.na(l1_src) & is.na(l2_src),
                             str_extract(user_population,  "(?<=\\().+?(?=\\))"),
                             NA_character_),
         total_yr = if_else(is.na(total_yr),
                            str_extract(total_src, "\\d+") %>% as.integer,
                            total_yr)
  )

## Subset key variables
ethno_sub <- ethno %>%
  select(country, eng_total, total_yr)

# Corruption
cpi <- cpi_raw %>%
  clean_names() %>%
  rename_with(~if_else(str_count(., "_") == 2, sub("_", "", .), .)) %>% # remove first _ for those with two
  pivot_longer(c(4:ncol(.)),
               names_to = c(".value", "year"),
               names_pattern = "(.+)_(.+)") %>%
  transmute(country, year = as.double(year), cpiscore)

# UN population estimates
pop <- pop_raw %>%
  clean_names() %>%
  rename(country = location, year = time) %>%
  mutate(across(c(7:10), ~.*1000)) %>%
  filter(variant == "Medium") %>%
  filter(year %in% c(1989:2021)) %>%
  select(country, year, pop_total)

# GDP PPP data
gdp_ppp <- gdp_ppp_raw %>%
  pivot_longer(c(5:ncol(.)), names_to = "year", values_to = "gdp_ppp") %>%
  clean_names() %>%
  filter(gdp_ppp != "X66") %>%
  transmute(country = country_name, year = as.integer(year), gdp_ppp)

###### Bind
supp <- cpi %>%
  full_join(ethno_sub, by = c("country", "year" = "total_yr")) %>%
  left_join(pop, by = c("country", "year")) %>%
  left_join(gdp_ppp, by = c("country", "year")) %>%
  mutate(eng_prop = eng_total/pop_total,
         gdp_ppp_pc = gdp_ppp/pop_total
  ) %>%
  filter(!is.na(eng_prop))



```

```{r format-get-help}
###### Format and bind get help data

###### Clean
# Geocodes
cap_geo <- cap_geo_raw %>%
  clean_names() %>%
  mutate(geocode = paste0(capital_latitude, ",", capital_longitude, ",20km")) %>%
  select(country = country_name, capital = capital_name, geocode)

# Leadership from REIGN
reign <- reign_raw %>%
  clean_names() %>%
  select(country, leader, year, month) %>%
  filter(lag(leader) != leader | lead(leader) != leader) %>%
  mutate(date_type = if_else(lead(leader) != leader, "end", "start"),
         date = paste0(year, "-", str_pad(month, 2, "left", pad = "0"), "-01"),
         date = as.Date(date),
  ) %>%
  filter(!(year < 2006 & date_type == "end" | date_type == "start" & lead(year) < 2006)) %>% # drop terms before 2006 (Twitter's founding)
  select(country, leader, date_type, date) %>%
  group_by(country, leader, date_type) %>%
  mutate(term_n = paste0("term ", 1:n())) %>% # count terms per leader NEED TO ADJUST FOR NAMES, e.g. LÃ¸kke, father-son?
  ungroup() %>%
  pivot_wider(names_from = date_type, values_from = date) %>%
  mutate(start = if_else(is.na(start), end, start),
  ) %>%
  rowwise() %>%
  mutate(date = list(seq.Date(start, end, by = "day"))) %>%
  tidyr::unnest(date) %>%
  mutate(country = case_when(country == "USA" ~ "United States",
                             TRUE ~ country),
         date_plus_one = date + days(1)) %>%
  filter(date >= as.Date("2006-07-15")) # when Twitter full version went live

# Proxy IPs
# proxy <- tibble(proxy = html_nodes(proxy_raw, "#proxylisttable td:nth-child(1)") %>% html_text(trim = TRUE),
#                 port = html_nodes(proxy_raw, "#proxylisttable td:nth-child(2)") %>% html_text(trim = TRUE),
#                 https = html_nodes(proxy_raw, "#proxylisttable td:nth-child(7)") %>% html_text(trim = TRUE)
# ) %>%
#   filter(https == "yes") %>% # only https types
#   transmute(proxy,
#             port,
#             proxy_type = "http",
#             proxy_no = row_number()
#             )

proxy <- read_tsv("https://api.proxyscrape.com/v2/?request=getproxies&protocol=http&timeout=100&country=all&ssl=all&anonymity=all&simplified=true") %>%
  rename(proxy = 1) %>%
  transmute(port = gsub(".+:", "", proxy) %>% as.integer,
            proxy = gsub("\\:.*", "", proxy),
            proxy_type = "http",
            proxy_no = row_number()
  )

###### Bind all together
get_help <- reign %>%
  left_join(cap_geo, by = "country") %>%
  mutate(proxy_no = rep(1:nrow(proxy), length.out = nrow(.))) %>% # repeat 1-300 to add proxy IPs
  left_join(proxy, by = "proxy_no") %>% # add rotating proxies
  mutate(port = as.integer(port))


```

# Get Tweets
```{r get-tweets-py}
###### Get Tweets with Python's Twint

###### Set up
# python version
use_python("/usr/local/bin/python3", required = TRUE)

# Source get_tweets Python function
source_python("src/py/get_tweets.py", convert = FALSE)

###### Get tweets and save as JSONs
# Testing
# get_tweets("Mnangagwa", "en", "-17.81666667,31.033333,20km",  5, "2020-01-01", "2020-01-05",
#            "data/test.json"
#            #"77.73.241.154", as.integer(8080), "http"
#            )

# Map across help data
get_help %>%
  filter(country == "Zimbabwe") %>%
  filter(date > as.Date("2020-01-01")) %>%
  #filter(date < as.Date("2020-01-10")) %>%
  mutate(csv_date = format(date, "%Y_%m_%d"),
         date = paste(date),
         date_plus_one = paste(date_plus_one),
  ) %>%
  mutate(tweets = pmap(list(leader, geocode, date, date_plus_one, country),
                       ~get_tweets(..1,
                                   "en",
                                   ..2,
                                   20,
                                   ..3,
                                   ..4,
                                   paste0("data/tweets/", ..1, "_", ..5, "_", ..3, ".json")
                                   # ..6,
                                   # ..7,
                                   # ..8
                       )
  )
  )

```

# Load Tweets
```{r load-tweets}
###### Load all Tweets
# Circa size of Tweets
file.size("data/tweets")/1000 # mb

# Function to read in and add file name as column
read_tweets_back <- function(csv){
  
  df <- stream_in(csv) %>%
    as_tibble()
  
  df$filename <- csv
  
  df
  
}

#read_tweets_back("data/tweets/Buhari2018-01-03.json")

# Map across function load all Tweets
tweets_raw <- list.files("data/tweets/", full.names = TRUE) %>%
  map_df(~read_tweets_back(.))

```


# Format Tweets
```{r format-tweets}
###### Tidy and format tweets
tweets <- tweets_raw %>%
  # select(contains("hashtags.")) %>%
  # view()
  unite(col = "hashtags_", contains("hashtags."), na.rm = TRUE) %>%
  unite(col = "photos_", contains("photos."), na.rm = TRUE) %>%
  unite(col = "urls_", contains("urls."), na.rm = TRUE) %>%
  relocate(matches("//d|0"), .after = last_col()) %>%
  mutate(filename,
         date = as.Date(date),
         country = gsub(".*_(.*)_.*", "\\1", filename),
         leader = gsub(paste0(".*//(.*)_", country, ".*"), "\\1", filename)
  )%>%
  clean_names() %>%
  select(date, name, country, leader, place, everything(),-c(contains("mentions"), contains("reply"), "filename"))

tweets

names(tweets)

```




# Sentiment analysis
```{r sentiment-analysis}
###### Sentiment analysis of Tweets


```

```{r extract-covariates}
###### Extract individual-level covariates


```

# Plots

```{r plot-eng-prop}
###### Plot English speaking population against corruption index
supp %>%
  filter(gdp_ppp_pc < 20000) %>%
  ggplot(.,
         aes(x = gdp_ppp_pc,
             y = eng_prop)) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "English speakers of countries with a GDP PPP PC below 20K")



```

```{r plot-tweet-freq}
###### Plot number of tweets every week
tweets %>%
  filter(country == "Nigeria") %>%
  mutate(week = floor_date(date, unit = "week"),
         month = floor_date(date, unit = "month")
  ) %>%
  group_by(month, country, leader) %>%
  summarise(n = n()) %>%
  ggplot(.,
         aes(x = month,
             y = n)) +
  geom_col() +
  labs(title = "Number of Tweets mentioning Buhari in Lagos, Nigeria",
       subtitle = paste0(nrow(tweets), " tweets from Lagos")
  ) 


```

```{r map-points}
###### Map Tweets as points
world1 <- map("world", plot = FALSE, fill = TRUE) %>% sf::st_as_sf()
sf::st_as_sf(map("africa", plot = FALSE, fill = TRUE))


###### Nigeria
nigeria_sf <- world1 %>% filter(ID == "Nigeria")

tweets_n <- nrow(tweets)
tweets_n_w_points <- nrow(tweets[!is.na(tweets$place_coordinates_0),])

ggplot() +
  geom_sf(data = nigeria_sf) +
  coord_sf(xlim = c(8.5, 9.5), ylim = c(7, 7.7), expand = FALSE) +
  geom_point(data = tweets, aes(x = place_coordinates_0, y = place_coordinates_1),
             size = 0.7) +
  labs(title = "Lagos in Nigeria: Tweets which included point spatial data",
       subtitle = paste0(tweets_n_w_points, " of ", tweets_n, " tweets include spatial data")
  ) +
  theme_bw()

ggmap::get_map(location = c(lon = -63.247593, lat = 17.631598), zoom = 14, maptype = "satellite")
qmap('Liverpool')


```

