---
title: "dev-public-opinion"
author: "William Rohde Madsen"
output: html_document
---

# Set up
```{r set-up}
# Load packages
library(tidyverse)
library(devtools)
library(readxl)
library(lubridate)
library(stringr)
library(janitor)
library(rgdal) # for shapefiles
library(sf) # manipulating spatial data
library(lme4) # for glmer, etc. modelling
library(httr)
library(ggrepel)
library(reticulate) # using python code in R
library(vroom) # loading data faster
library(ndjson)
library(jsonlite)
library(rvest) # reading html websites
library(maps)
library(ggmap)
library(stars) # rasters
library(maptools)
library(spatstat)
library(koRpus)
library(quanteda)
library(textdata)
library(tidytext)
library(SnowballC)

# Source Python twint function
use_python("/usr/local/bin/python3", required = TRUE)

source_python("src/py/get_tweets.py", convert = FALSE)

```

# Load prepared data
```{r load-data}
load("data/formatted_data.RData")
load("data/tweets_formatted.RData")
```

# Get and save tweets
```{r scraping-data}
# Choose how long back you want to scrape data for losing election candidates
people_to_scrape <- candidates %>%
  ungroup() %>%
  # eg beginning of month two months ago
  mutate(elex_start = (elex_date - months(2)) %>% floor_date(., unit = "month")
  ) %>%
  select(country, name, start = elex_start, end = elex_date)

# Create object with scraping frequency and names
scrape_freq_names <- create_scrape_freq(reign, people_to_scrape, days = 7)

# Create smallest possible circles
small_circs <- create_smallest_possible(gadm)

# Find radius and centre coordinates
centre_and_radius <- find_centre_and_radius(small_circs)

# Create geocodes
geocodes <- centre_and_radius %>%
  transmute(country, geocode = paste0(x, ",", y, ",", radius_m/1000, "km"))

# Join data
scrape_data <- scrape_freq_names %>%
  left_join(geocodes, by = "country") %>%
  filter(!is.na(geocode)) %>%
  arrange(name, desc(date)) # descending to get recent tweets first

```

```{r get-tweets, eval = FALSE}
# Get tweets without points
get_tweets_r(scrape_data, path = "data-raw/tweets/without/", limit = FALSE, include_geocode = FALSE)

# Get tweets with points
get_tweets_r(scrape_data, path = "data-raw/tweets/without/", limit = FALSE, include_geocode = TRUE)


```

```{r load-tweets, eval = FALSE}
# Map across function load all Tweets across hundreds of JSONs
tweets_raw_list <- list.files("data-raw/tweets",
                              full.names = TRUE, recursive = TRUE) %>%
  purrr::map(~read_tweets_back(.))

# Bind tweets into dataframe
tweets_raw <- bind_raw_tweets(tweets_raw_list)

# Format and add variables, including leader and country
tweets_formatted <- format_tweets(tweets_raw, candidates)

# Add point data if possible

# Add region data and other covariates (add candidates here?)

# Save as single object
save(tweets_formatted, file = "data/tweets_formatted.csv")


```

# Sentiment analysis
```{r prepare-text}
# Clean from patterns and turn to lower-case
tweets_clean <- tweets_formatted %>%
  mutate(tweet = tolower(tweet),
         tweet = remove_patterns_in_tweet(tweet),
         tweet = str_squish(tweet) # removes repeated white space
  )

# Convert to tokens and stem
tweets_tokens <- tweets_clean %>%
  unnest_tokens(word, tweet)

# Stem and find stop words
tweets_tokens <- tweets_tokens %>%
  mutate(word = wordStem(word),
         stopword = if_else(word %in% get_stopwords(language = "en", source = "snowball"), TRUE, FALSE)
         )

get_stopwords(language = "english", source = "snowball")

```

```{r calculate-sentiment}
# Add lexicons for sentiment analysis


```


# Train model
```{r train-model}

```


# Plots
```{r}
# Simply GADM for plotting ease
gadm_simp <- gadm %>%
  st_simplify(., dTolerance = 0.1)

# Plots

# English-speakers plot ------
# Plot different data on English speakers
supp %>%
  ggplot(.,
         aes(x = eng_prop_wiki,
             y = eng_prop) # ethno
  ) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "Different sources on English speakers per country")

# Plot English speaking population against corruption index
supp %>%
  #filter(wgi_est < 1) %>%
  filter(twitter_users_pc < 0.3) %>%
  ggplot(.,
         aes(x = twitter_users_pc,
             y = eng_prop)) +
  geom_point() +
  geom_label_repel(aes(label = paste0(country, " ", year))) +
  labs(title = "English-speaking share by voice and accountability index")

# Raw tweets plots -----
# Plot number of tweets every week
tweets %>%
  filter(country == "Nigeria") %>%
  mutate(week = floor_date(date, unit = "week"),
         month = floor_date(date, unit = "month")
  ) %>%
  group_by(month, country, leader) %>%
  summarise(n = n()) %>%
  ggplot(.,
         aes(x = month,
             y = n)) +
  geom_col() +
  labs(title = "Number of Tweets mentioning Buhari in Lagos, Nigeria",
       subtitle = paste0(nrow(tweets), " tweets from Lagos")
  ) 


# Map Tweets as points
world1 <- map("world", plot = FALSE, fill = TRUE) %>% sf::st_as_sf()
sf::st_as_sf(map("africa", plot = FALSE, fill = TRUE))

# Nigeria NE
nigeria_sf <- world1 %>% filter(ID == "Nigeria")

tweets_n <- nrow(tweets)
tweets_n_w_points <- nrow(tweets[!is.na(tweets$place_coordinates_0),])

ggplot() +
  geom_sf(data = nigeria_sf) +
  coord_sf(xlim = c(8.5, 9.5), ylim = c(7, 7.7), expand = FALSE) +
  geom_point(data = tweets, aes(x = place_coordinates_0, y = place_coordinates_1),
             size = 0.7) +
  labs(title = "Lagos in Nigeria: Tweets which included point spatial data",
       subtitle = paste0(tweets_n_w_points, " of ", tweets_n, " tweets include spatial data")
  ) +
  theme_bw()


# Methodology plots -----
# Plot scraper circles
ggplot() +
  geom_sf(data = gadm_simp) +
  geom_sf(data = gadm_circ, fill = NA)


```

