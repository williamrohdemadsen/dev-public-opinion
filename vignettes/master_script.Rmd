---
title: "dev-public-opinion"
author: "William Rohde Madsen"
output: html_document
---

# Set up
```{r set-up}
# Load packages
library(tidyverse)
library(readxl)
library(lubridate)
library(stringr)
library(janitor)
library(rgdal) # for shapefiles
library(sf) # manipulating spatial data
library(lme4) # for glmer, etc. modelling
library(httr)
library(ggrepel)
library(reticulate) # using python code in R
library(vroom) # loading data faster
library(ndjson)
library(quanteda)
library(jsonlite)
library(tidytext)
library(rvest) # reading html websites
library(maps)
library(ggmap)
library(stars) # rasters
library(maptools)
library(spatstat)
library(textdata)

# Source Python twint function
use_python("/usr/local/bin/python3", required = TRUE)

source_python("src/py/get_tweets.py", convert = FALSE)

```

# Load prepared data
```{r load-data}
load("data/formatted_data.RData")
```

# Format scraping data
```{r get-tweets}
source("src/R/03_format_scraping_data.R")

# Choose how long back you want to scrape data for losing election candidates
people_to_scrape <- candidates %>%
  ungroup() %>%
  # eg beginning of month two months ago
  mutate(elex_start = (elex_date - months(2)) %>% floor_date(., unit = "month")
  ) %>%
  select(country, name, start = elex_start, end = elex_date)

# Create object with scraping frequency and names
scrape_freq_names <- create_scrape_freq(reign, people_to_scrape, days = 7)

# Create smallest possible circles
small_circs <- create_smallest_possible(gadm)

# Find radius and centre coordinates
centre_and_radius <- find_centre_and_radius(small_circs)

# Create geocodes
geocodes <- centre_and_radius %>%
  transmute(country, geocode = paste0(x, ",", y, ",", radius_m/1000, "km"))

# Join data -----
scrape_data <- scrape_freq_names %>%
  left_join(geocodes, by = "country") %>%
  filter(!is.na(geocode)) %>%
  arrange(name, desc(date)) # descending to get recent tweets first

```

# Get tweets
```{r get-tweets, eval = FALSE}
source("src/R/04_get_tweets.R")

# Get tweets without points
get_tweets_r(scrape_data, path = "data-raw/tweets/without/", limit = FALSE, include_geocode = FALSE)

# Get tweets with points
get_tweets_r(scrape_data, path = "data-raw/tweets/without/", limit = FALSE, include_geocode = TRUE)


```

# Load raw tweets
```{r load-tweets, eval = FALSE}
source("src/R/05_load_tweets.R")
source("src/R/06_format_tweets.R")

# Map across function load all Tweets across hundreds of JSONs
tweets_raw_list <- list.files("data-raw/tweets",
                              full.names = TRUE, recursive = TRUE) %>%
  purrr::map(~read_tweets_back(.))

# Bind tweets into dataframe
tweets_raw <- bind_raw_tweets(tweets_raw_list)

# Format and add variables, including leader and country
tweets_formatted <- format_tweets(tweets_raw, candidates)

# Save as single object
save(tweets_formatted, file = "data/tweets_formatted.csv")


```

# Load tweets
```{r format-tweets}
load("data/tweets_raw.RData")
```

# Sentiment analysis

# Plots
```{r}
# Simply GADM for plotting ease
gadm_simp <- gadm %>%
  st_simplify(., dTolerance = 0.1)

```

